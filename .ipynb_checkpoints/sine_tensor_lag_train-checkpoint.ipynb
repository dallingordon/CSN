{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "b35ceec7-85e7-4618-a2bc-ef622825a0cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import random\n",
    "\n",
    "\n",
    "def count_duplicates(tensor):\n",
    "    # Check the shape of the tensor\n",
    "    if tensor.dim() != 2:\n",
    "        raise ValueError(\"Input tensor must be 2-dimensional (length x bits).\")\n",
    "\n",
    "    # Convert the tensor to a set of tuples to find unique rows\n",
    "    unique_rows = set(map(tuple, tensor.tolist()))\n",
    "    \n",
    "    # Calculate the number of duplicates\n",
    "    num_duplicates = tensor.size(0) - len(unique_rows)\n",
    "    \n",
    "    return num_duplicates\n",
    "\n",
    "\n",
    "\n",
    "def generate_sine_tensor(num_bits, length):\n",
    "    # Create an array of integers from 0 to length - 1\n",
    "    t = np.arange(length)\n",
    "    # Generate the sine waves for each bit\n",
    "    sine_tensor = np.zeros((length, num_bits))  # Initialize the tensor\n",
    "    \n",
    "    for i in range(num_bits):\n",
    "        frequency = (np.pi / (2 ** i))  # Calculate frequency based on the number of bits\n",
    "        sine_tensor[:, i] = np.sin(frequency * (t+0.5))  # Fill the tensor with sine values\n",
    "\n",
    "    return sine_tensor\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "03e26e77-d745-4b99-b6d1-bca157e08357",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def rand_repeats(length,repeat_max = 1000,abs_val_max = 1000):\n",
    "    output = []\n",
    "    while len(output) < length:\n",
    "        val = (random.random()-0.5)*2*abs_val_max\n",
    "        repeats = random.randint(0,1000)\n",
    "        output.extend([val]*repeats)\n",
    "    output = output[:length]\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "039eac82-d7b6-43f9-bf9f-8817e31a430a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class OnlyPhiLayer(nn.Module):\n",
    "    def __init__(self, num_bits):\n",
    "        super(OnlyPhiLayer, self).__init__()\n",
    "        self.num_bits = num_bits\n",
    "\n",
    "        self.denominators = torch.tensor([2 ** i for i in range(num_bits)])\n",
    "\n",
    "\n",
    "    def forward(self, t, phi):\n",
    "        t_phi_b = torch.full((self.num_bits,), t + phi)\n",
    "        \n",
    "        # Compute the sine values using the sine tensor and the modified time\n",
    "        sine_values = torch.sin(torch.pi/self.denominators * t_phi_b)\n",
    "        return sine_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "949485d8-617b-476d-ae40-d03114b662d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = OnlyPhiLayer(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "c8002cf9-2797-4455-af1d-78e42afa7de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_len = 50000\n",
    "bits = 15\n",
    "\n",
    "\n",
    "num_bits = bits # Change this for different number of bits\n",
    "length = data_len   # Change this for different lengths\n",
    "result_tensor = generate_sine_tensor(num_bits, length)\n",
    "result_tensor = torch.tensor(result_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "2e8c7486-af3e-4a8d-8e1c-96ce50ed3ec2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_duplicates(result_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "338cde35-ab62-4e54-95d3-92d9c5ece004",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([49959, 15]) torch.Size([49959, 15])\n"
     ]
    }
   ],
   "source": [
    "off_set = 41\n",
    "test_in = result_tensor[:-off_set].float()\n",
    "test_out = result_tensor[off_set:].float()\n",
    "print(test_in.shape, test_out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "28dbe17f-ca47-4c7a-ad0c-c5cd00211629",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([49959])\n"
     ]
    }
   ],
   "source": [
    "t = rand_repeats(test_in.shape[0])\n",
    "test_out = torch.tensor(t).float()\n",
    "print(test_out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "aebb9e64-f8a3-4c67-bfcd-20b47a3b1e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, inputs, outputs):\n",
    "        self.inputs = inputs\n",
    "        self.outputs = outputs\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.inputs[idx], self.outputs[idx]\n",
    "\n",
    "# Example input tensors\n",
    "# Assuming test_in and test_out are your input and output tensors\n",
    "# Make sure they are both tensors and have the same length\n",
    "# test_in = torch.randn(num_samples, input_dim)  # Example input\n",
    "# test_out = torch.randn(num_samples, output_dim)  # Example output\n",
    "\n",
    "# Create the dataset\n",
    "dataset = CustomDataset(test_in, test_out)\n",
    "\n",
    "# Create a DataLoader\n",
    "batch_size = 512  # Adjust as needed\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "7f00f393-2ddb-43b1-92fe-b115f474b2a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultiLayerModelOne(\n",
       "  (layers): ModuleList(\n",
       "    (0-3): 4 x Linear(in_features=15, out_features=15, bias=True)\n",
       "  )\n",
       "  (output_layer): Linear(in_features=15, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Define a model with a variable number of layers\n",
    "class MultiLayerModel(nn.Module):\n",
    "    def __init__(self, input_dim, num_layers):\n",
    "        super(MultiLayerModel, self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "\n",
    "        # Create the specified number of layers\n",
    "        for _ in range(num_layers):\n",
    "            self.layers.append(nn.Linear(input_dim, input_dim))  # Each layer has the same input and output dimension\n",
    "\n",
    "    def forward(self, x):\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            x = layer(x)  # Pass through the layer\n",
    "            if i < len(self.layers) - 1:  # Apply ReLU activation only for all but the last layer\n",
    "                x = F.relu(x)\n",
    "        return x  # Apply sigmoid activation at the end\n",
    "\n",
    "\n",
    "class MultiLayerModelOne(nn.Module):\n",
    "    def __init__(self, input_dim, num_layers):\n",
    "        super(MultiLayerModelOne, self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "\n",
    "        # Create the specified number of layers\n",
    "        for _ in range(num_layers - 1):  # Create num_layers - 1 linear layers\n",
    "            self.layers.append(nn.Linear(input_dim, input_dim))  # Each layer has the same input and output dimension\n",
    "        \n",
    "        # Add the last layer that reduces output to a single scalar\n",
    "        self.output_layer = nn.Linear(input_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            x = layer(x)  # Pass through the layer\n",
    "            if i < len(self.layers) - 1:  # Apply ReLU activation only for all but the last layer\n",
    "                x = F.relu(x)\n",
    "        \n",
    "        x = self.output_layer(x)  # Pass through the output layer to get a scalar\n",
    "        return x # Return the scalar output\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Example usage\n",
    "input_dim = test_in.shape[1]\n",
    "num_layers = 5  # Specify the number of layers\n",
    "\n",
    "model = MultiLayerModelOne(input_dim, num_layers)\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "44416880-05f4-4fdb-8dad-c7cb2c440c58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/5000], Loss: 252471.3750\n",
      "Epoch [20/5000], Loss: 226669.0938\n",
      "Epoch [30/5000], Loss: 201862.0000\n",
      "Epoch [40/5000], Loss: 183860.5000\n",
      "Epoch [50/5000], Loss: 140292.7656\n",
      "Epoch [60/5000], Loss: 132293.4219\n",
      "Epoch [70/5000], Loss: 145445.2812\n",
      "Epoch [80/5000], Loss: 125662.1562\n",
      "Epoch [90/5000], Loss: 129546.3828\n",
      "Epoch [100/5000], Loss: 146501.9219\n",
      "Epoch [110/5000], Loss: 148215.2344\n",
      "Epoch [120/5000], Loss: 140836.7812\n",
      "Epoch [130/5000], Loss: 129336.6953\n",
      "Epoch [140/5000], Loss: 122501.0156\n",
      "Epoch [150/5000], Loss: 129747.8516\n",
      "Epoch [160/5000], Loss: 107608.0312\n",
      "Epoch [170/5000], Loss: 128872.5391\n",
      "Epoch [180/5000], Loss: 119287.3594\n",
      "Epoch [190/5000], Loss: 121061.3672\n",
      "Epoch [200/5000], Loss: 119936.4062\n",
      "Epoch [210/5000], Loss: 108073.5469\n",
      "Epoch [220/5000], Loss: 102026.4141\n",
      "Epoch [230/5000], Loss: 115573.1406\n",
      "Epoch [240/5000], Loss: 92241.6172\n",
      "Epoch [250/5000], Loss: 117227.4844\n",
      "Epoch [260/5000], Loss: 103949.3594\n",
      "Epoch [270/5000], Loss: 99920.9766\n",
      "Epoch [280/5000], Loss: 116201.3984\n",
      "Epoch [290/5000], Loss: 111134.2188\n",
      "Epoch [300/5000], Loss: 123558.3984\n",
      "Epoch [310/5000], Loss: 119215.7266\n",
      "Epoch [320/5000], Loss: 104297.5703\n",
      "Epoch [330/5000], Loss: 108262.8906\n",
      "Epoch [340/5000], Loss: 110502.6875\n",
      "Epoch [350/5000], Loss: 93702.0312\n",
      "Epoch [360/5000], Loss: 102028.6484\n",
      "Epoch [370/5000], Loss: 112238.8359\n",
      "Epoch [380/5000], Loss: 100824.7969\n",
      "Epoch [390/5000], Loss: 106554.0781\n",
      "Epoch [400/5000], Loss: 91249.7812\n",
      "Epoch [410/5000], Loss: 116979.8125\n",
      "Epoch [420/5000], Loss: 103481.7656\n",
      "Epoch [430/5000], Loss: 97024.3906\n",
      "Epoch [440/5000], Loss: 112809.6562\n",
      "Epoch [450/5000], Loss: 122492.9609\n",
      "Epoch [460/5000], Loss: 107650.5625\n",
      "Epoch [470/5000], Loss: 111254.2500\n",
      "Epoch [480/5000], Loss: 108164.1094\n",
      "Epoch [490/5000], Loss: 98916.9297\n",
      "Epoch [500/5000], Loss: 123423.0078\n",
      "Epoch [510/5000], Loss: 100354.5781\n",
      "Epoch [520/5000], Loss: 110773.6953\n",
      "Epoch [530/5000], Loss: 102496.7031\n",
      "Epoch [540/5000], Loss: 111165.7969\n",
      "Epoch [550/5000], Loss: 115653.3125\n",
      "Epoch [560/5000], Loss: 94317.1562\n",
      "Epoch [570/5000], Loss: 111271.5312\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[151], line 21\u001b[0m\n\u001b[1;32m     19\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(outputs, batch_out\u001b[38;5;241m.\u001b[39mfloat())\n\u001b[1;32m     20\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()  \u001b[38;5;66;03m# Backward pass\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m     \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Update weights\u001b[39;00m\n\u001b[1;32m     23\u001b[0m     losses\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mitem())  \u001b[38;5;66;03m# Store loss\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (epoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:  \u001b[38;5;66;03m# Print every 100 epochs\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/optim/optimizer.py:385\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    380\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    381\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    382\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    383\u001b[0m             )\n\u001b[0;32m--> 385\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    386\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    388\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/optim/optimizer.py:76\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     75\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[0;32m---> 76\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     78\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/optim/adam.py:166\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    155\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    157\u001b[0m     has_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[1;32m    158\u001b[0m         group,\n\u001b[1;32m    159\u001b[0m         params_with_grad,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    163\u001b[0m         max_exp_avg_sqs,\n\u001b[1;32m    164\u001b[0m         state_steps)\n\u001b[0;32m--> 166\u001b[0m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    168\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    169\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    184\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    185\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    186\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    187\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/optim/adam.py:316\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    313\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    314\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[0;32m--> 316\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    317\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    318\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    319\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    320\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    321\u001b[0m \u001b[43m     \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    322\u001b[0m \u001b[43m     \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    323\u001b[0m \u001b[43m     \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    324\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    325\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    326\u001b[0m \u001b[43m     \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    327\u001b[0m \u001b[43m     \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    328\u001b[0m \u001b[43m     \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    329\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    330\u001b[0m \u001b[43m     \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    331\u001b[0m \u001b[43m     \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    332\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    333\u001b[0m \u001b[43m     \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/optim/adam.py:439\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    437\u001b[0m         denom \u001b[38;5;241m=\u001b[39m (max_exp_avg_sqs[i]\u001b[38;5;241m.\u001b[39msqrt() \u001b[38;5;241m/\u001b[39m bias_correction2_sqrt)\u001b[38;5;241m.\u001b[39madd_(eps)\n\u001b[1;32m    438\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 439\u001b[0m         denom \u001b[38;5;241m=\u001b[39m (\u001b[43mexp_avg_sq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbias_correction2_sqrt\u001b[49m)\u001b[38;5;241m.\u001b[39madd_(eps)\n\u001b[1;32m    441\u001b[0m     param\u001b[38;5;241m.\u001b[39maddcdiv_(exp_avg, denom, value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39mstep_size)\n\u001b[1;32m    443\u001b[0m \u001b[38;5;66;03m# Lastly, switch back to complex view\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Criterion and optimizer\n",
    "criterion = nn.MSELoss()  # Change if needed\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 5_000  # Adjust as needed\n",
    "losses = []  # To store loss values for later analysis\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # Set the model to training mode\n",
    "\n",
    "    for batch_in, batch_out in dataloader:\n",
    "        optimizer.zero_grad()  # Clear gradients\n",
    "        \n",
    "        # Ensure inputs are float\n",
    "        outputs = model(batch_in.float()).squeeze()  # Forward pass\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = criterion(outputs, batch_out.float())\n",
    "        loss.backward()  # Backward pass\n",
    "        optimizer.step()  # Update weights\n",
    "\n",
    "        losses.append(loss.item())  # Store loss\n",
    "        \n",
    "    if (epoch + 1) % 10 == 0:  # Print every 100 epochs\n",
    "        print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "# After training, evaluate your model on validation data if needed\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "with torch.no_grad():\n",
    "    val_outputs = model(validation_data.float())  # Example for validation\n",
    "    # Evaluate performance on validation data here\n",
    "\n",
    "# Optionally, plot the losses\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(range(len(losses)), losses)\n",
    "plt.xlabel('Batch Iteration')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss over Batches')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fced09c-d070-4df4-83f9-904b1839f673",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
