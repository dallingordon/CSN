9/16

okay we have a small data set as well as a large one.  old stuff (llama) is cleaned up.  built dgwave.  puttins this stuff there. fix the configs and start testing those out on the scc.  



9/7
okay first of all, what i think he meant by the other way is more impressive the generation, it is the emergent capacity.  to say shit it wasn't trained on.  

i want to add in a sentence (or start with word) embedding.  i am thinking of using a layer of llama toward the end.  pass that in at every t, and train it to output not just the integer for the audio but a bit as well of when to end.  put some ones at the very end so it knows to stop generating.  

first try it with like, 10 words.  no emergence yet.  get digits 0-9 from google, and also the embeddings from llama, train it, and see if you can get a single model to make all 10 numbers intelligibly.  

then string numbers together, each audio file is between 1 and say, 5 digits.  then see if it can produce files it wasn't trained on.  



8/28

epsilon? for when something is zero?  like, it may make sense to deaden layers maybe? what if phi is zero? how common is that?

also super super sine can learn nothing, as in, input = output.  Is this a NAS thing? can i "deaden" or, uselessify a layer.  well obvs i can, i mean can i do it in an advantageous way.

got that working well it seems.  i need logging.  i need inference (see lots of graphs, how do i wanna do that)
i need to save latest and best weights.  

I think i want a scheduler thing in the config, as well as an optimizer to do param groups, change the lr by layer.  maybe even by type.  like, phi's change different from a's or cs.  ponder that.  

8/26
added silence to the beginning and end.  is that difficult? i am starting to wonder if it needs help doing really high frequecny stuff?

the final layer goes apart? i wonder if that is common? i like that the absolute amplitude is increasing, that makes sense, but yeah, weirdy.  

not looking good, the noise shit.  that is to say, it doesn't learn zero well.  that flat line, it looks like the max amplitude in some way.  it must be something to do with all the layers? there is an effective max and min?

it also looks like some early layers get higher amplitudes? that is surprising.  

sine chain, what if each layer gets pushed through an amplitude 1 sine.  nice and normalized at every "layer"



8/12

added a final sine, nothing wildly improved.   

init x 10 for the amplitude params and it stalled.  do the opposite now,,,

yeah, it looks like it can start low and learn higher.  i wonder if it makes sense to only increase the params? that isn't quite right but it seems to get to a place where it can't learn the highes extremes.  that might be a loss issue. it won't learn the one or 2 random edges cuz they get averaged out? 

how do you make it MORE susceptible to extremes? 

batch size from 100 to 20.  no major difference.  

is there a way to, say we do a few consecutive, can we weight it higher if there are more swaps from positive to negative? idk how.

okay, more consecutive points worked better.  thats interesting.  

these ablations were all with lr=0.005

i think i want to add the long distance ones too.  so right now it is just consecutive, upping num_consec seems to improve the acc

with 50 i let it run to 400 epochs (best was with 20 consec, batchsize 100, bla bla.  
50 didn't seem to be an improvement.  trying my finalWave with 20 consec,  not it. i think i may have gotten lucky that one time.  

what if you also had some random ones, compare those slopes? do it seperately? like, another data loader? idk.  

also, mess with dif in dif.  right now it is slopes.  what about the dif in those

and of coure the network that has 2 params per layer, and it takes in t original, and the output from the previous


okay, the chain one, make it so at each level you can add in a number of waves, those get passed to the next as t, and added again. 
so, 10 layers, if the input to that is 5, you should have 2 x 5 x 10 so 100 params.  that seems right.  
trying with 10:
no, with the final thing: 

_________________
8/8. been rough

i made 2dDataLoader.ipynb which lets you randomly sample and get consecutive datapoints.  slope stuff has worked.  


__________________________
8/3
adding in a phi helped lots i think. 
what if instead of a triangle i make a point on the top and bottom of the wave that is zero.  don't mess with the derivatives, but give it a spot that it can learn zero, near the tip.  try it

Also, training on a higher frequency, i think it just takes longer to fill in those? it looks like its working.


mess with the trainer i think.  random points, then stuff near them. don't do all difs, and if you do say, x points around the sample, weight it if you want.  


high frequencies aren't being addressed well it seems.  i guess thos oscilations happen more, i wonder if bigger samples (given how i am training) affects if you use a batch wide loss.  might make sense to just look at neighbors.  if you get the local slopes (+/- 1 from the sample t) eventually that makes for the whole thing being correct.  emergent? 
_______
it does well with the most dominant frequency, lower ones struggle. the loss function i think is the real champ.  
messed with CD with the slopes, that seems to accomplish basically the same thing.

we could make batch area dim into something. care more about the distance? if you pay more attention to the big ones first, that makes sense.  you have to get the far distances first, then do the small ones?  mess with that.  
__________________________________________________________________________________________________________
8/1
PROXIMAL DIF

next upgrade, i think because of long range frequencies? or just long patterns. randomly do long range loss, do the same as you did inside a batch, but look longer range.  

also, this worked for the linear and sin.  i used mse and proximal diff loss (pdl)  also, t is changed to np.linspace and i spaced it out between like, 4 and 7 so that all the values are unique and its a wave, half a phase of it.

for the minimal param model, weight it so a zero is penalized? yeah.  i don't want them small. like, below zero.

It starts learning in the center, then works out.  i wonder if my initial t is doing what i want?



7/25
____
lots to look at.  adding it at the end of another thing is a good idea.  all the hyper params, learning, init, b la bla.

_____
7/24
momentum.  when does it start
model is resetting?
    which one? teacher student?
    
is the training procedure different
    how does it differ
    
gotta be that your starting to "model averaging"
    exponential model averaging?
    
gotta be the weights.  the weights changed.  
    
is this training the student from scratch
cheating?
see what the defaults are.  warm up is suspicious
training a piece and then train the whole thing?


_____
7/23
it is killing the kernel at L=6 which is wild, that isn't that many.  scc time?
it looks like it can learn with mse, not a perfect fit, but its only with L=5.  
training is weird, i suspect gradients are at different scales, graph those by layer and type.  c is probably fine but my guess is the gradients are wiggling the mose for the lowest frequency layers.  prove that.

yeah, it will learn at a particular rate, and then INCREASING it at higher epochs lets it keep training.  something is getting to zero, and then other crap keeps learning? gradients.

the later layer grads are 100 to 1000 times smaller? graph

everything is converging on zero.........

i wrote it in double dutch, but what if we only have the final wave do amplitude? all the highest frequency things spit out the readding.  sum that whole layer to get the pred for that t instead of how it is now, each frequency spitting out a guess per denominator per t.  try that.


i think i need to do the activation test first.  ones and zeros.  no a.  cool

also.  is there a way to make a 1/0 target with that sigmoid thing?

add another one with a learnable c, and apply it to the target? the same c? idk.  weird.  

i think i need to make it so that the only branch that gets updated is the one with the activations.  so, forward prop the activations when you do grad updates? thats interesting.  



__________________________________________________
7/22
i don't think they can share weights honey.   make em individual.  that is still way low

ok, first it was doing real good at estimating a single non explicit frequency.  that is major.  

working on the non-linearity.  the shapes aren't doing what i think.  figure that bit out, the c's in there are being shared and broadcast.  i think what i saw with the single wave is the a's are unique, but the c's aren't.  so they all learn the same activation pattern at every level? that makes sense to me.  attack that. gj.  you did good today.


_______________________

7/21
original method didnt' work.  not surprising, added the waves at each layer, then. went to next.  i need branching

i might need to just parameterize the activation? move where the cutoff is? so i can do the 1s and 0s and all that?

each layer needs to learn its own amplitude, and that is added? at the end? yeah.  but it also needs to get passed through an activation that subsequent waves get selected with? or does every wave go through to the end? 



____________
7/20
hi sad boy.  

ok, double dutch layer.  takes a denominator, divides the sampling rate by that denom, and makes waves at opposite ends.  if you feed those into layers with succesivelye smaller denominators, you get binary double dutch.  

the amplitutdes.  if they are equal, they should cancel out.  that works.  if you make one higher than the other you get phase offsets.  

the other possible thing would be 2 sigmoids with positive 1 and negative 1.  not sure about that.  oooh actually, that and then a seperate param for that layers amplitude.  train and test babay.  
___________________________________________________________
7/16
sine wave layer higher has higher than sampled frequencies, the idea being since they only manifest discretely they can model different shapes.  interesting idea.  didn't work well with simple stuff.  
with complex stuff 

even with higher (than sample rate freq) waves it isn't filling in the gaps.  it can converge with perfect data, but estimating the non supplied freq is hard.  

i have SineWaveLayerInc working, and it seems to learn slow, but still improving

questioning mse, i want it to multiply the wave if it works, but average may not be working.  i want to not worry about a few big mistakes.  idk pondering.
also, add the high frequency ones too.  

what if we just add the 2 losses together, train them both?

___________________________________________________________________________________
7/15
no from Glidance.  fuck em

okay, distance from the cost is important.  if you do all the same it gives them all the same value.  random init?
network built them multiple times, not sure that helped.

the phase loss is working really well. add layer i think i prefer to the binary one.  thats neat

make it so the highest frequency is farthest away.  it starts by doing the lowest frequency wave in back prop, then moves back, the highest freq being the last.  so, each layer takes the next highest, adds it to the previous everything.  

can i rig that to adjust based on a local loss somehow? make each one substract the max that it can?

what happens if you add higher than sampling rate frequencies? i wonder if that gives you more control? like, you can estimate non-explicit frequencies using high freq waves? what would that look like? and what are the edits it would add.  draw.  it can't just be double, that would be the same as adding nyquist right?



_______________________________________
7/11

nyqvist frequecny



______________
7/10
OKAY the binary wave network is working.  right now it can learn itself.  I added phase agreement which is just trying to line phase up, that seems to be working well.  

phi_center isn't working like i think, but it is converging, just to a different phi minima, which is inconsequential. 

if the LEARNED wave is more limited that the target wave, it didn't converge.  that i think is the next big question.  if i add more frequencies can it learn? what if i did multiple of these and add them after the fact? CAPACITY????

can it learn, or rather, how well can it approximate waves that are composed of other frequencies.

explain the compression thing to kulis.  


_____________________________________
7/4
axis agreement might work for tuning phase.  HOWEVER, it depends on a correct frequency, otherwise maximizing this is nonsese.  
 messing with surrogates.  slope of all points? might work better with a range that is all connected?
____________________________________________________________________________________________
6/19
SINUSOIDAL FREQUENCY ESTIMATION BY GRADIENT DESCENT Hayes
	The loss (he shoes MSE, MAE, DFT [discreet Fourier transform] are all highly non convex (he is doing a sum of sin waves, I don't think my think differs substantially to, no, not sure if mine differs enough, I gotta math]. It is frequency that is the issue, the loss wrt frequency is going to be sin wavy, and those derivatives are going to be uninformative
	
	DIFFERENTIABLE DIGITAL SIGNAL PROCESSING engel
		At the heart of the synthesis techniques explored in this paper is the sinusoidal oscillator. That sounds like my first layer.
They individually model things, reverb, 
	multi scale spectrogram loss is what they are using.  

	frequency is the issue.  How can you tune it? Like, it's always wrong until it is exactly right.  If you are half the frequency, how can it learn to get to 3/4 then the right frequency.  It

	

_________________________________
6/17

alright, got it training.  the "batch" is really just how many time steps you run through it at the time.  i need to spend some time making sure it is doing what i think. 

got it training.  running it with mse, really high losses.  only have activations (relu) on w and phi, not on amplitude.  not sure about that.  

training with very few layers (the SineWaveNetwork() in csn) doesn't do much.  making it deeper, giving it more small waves, and decreasing to the one output that we overfitting on (dog bark, monet is the dog that inspired it, ben's dog, if you marry him, if not, delete)

make configs next time.  
hyperparameters = {
    'learning_rate': 0.0001,
    'epochs': 100,
    'batch_size': 100,
}
with (6 layers, decreasing from 100 base) started learning somewhat consistently

also, check the passing of the phi and w.  that addition (as well as the weird way i want to do gd)

you want the biggest waves and the longest phi to change the fastest.  thats gd.  

i think i can do it even with the relus on there.  mess with it.  yeah,  pre relu there will be negatives, which will subtract, after the backward pass and step, readjust everything
