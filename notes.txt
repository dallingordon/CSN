it does well with the most dominant frequency, lower ones struggle. the loss function i think is the real champ.  
messed with CD with the slopes, that seems to accomplish basically the same thing.

we could make batch area dim into something. care more about the distance? if you pay more attention to the big ones first, that makes sense.  you have to get the far distances first, then do the small ones?  mess with that.  
__________________________________________________________________________________________________________
8/1
PROXIMAL DIF

next upgrade, i think because of long range frequencies? or just long patterns. randomly do long range loss, do the same as you did inside a batch, but look longer range.  

also, this worked for the linear and sin.  i used mse and proximal diff loss (pdl)  also, t is changed to np.linspace and i spaced it out between like, 4 and 7 so that all the values are unique and its a wave, half a phase of it.

for the minimal param model, weight it so a zero is penalized? yeah.  i don't want them small. like, below zero.

It starts learning in the center, then works out.  i wonder if my initial t is doing what i want?



7/25
____
lots to look at.  adding it at the end of another thing is a good idea.  all the hyper params, learning, init, b la bla.

_____
7/24
momentum.  when does it start
model is resetting?
    which one? teacher student?
    
is the training procedure different
    how does it differ
    
gotta be that your starting to "model averaging"
    exponential model averaging?
    
gotta be the weights.  the weights changed.  
    
is this training the student from scratch
cheating?
see what the defaults are.  warm up is suspicious
training a piece and then train the whole thing?


_____
7/23
it is killing the kernel at L=6 which is wild, that isn't that many.  scc time?
it looks like it can learn with mse, not a perfect fit, but its only with L=5.  
training is weird, i suspect gradients are at different scales, graph those by layer and type.  c is probably fine but my guess is the gradients are wiggling the mose for the lowest frequency layers.  prove that.

yeah, it will learn at a particular rate, and then INCREASING it at higher epochs lets it keep training.  something is getting to zero, and then other crap keeps learning? gradients.

the later layer grads are 100 to 1000 times smaller? graph

everything is converging on zero.........

i wrote it in double dutch, but what if we only have the final wave do amplitude? all the highest frequency things spit out the readding.  sum that whole layer to get the pred for that t instead of how it is now, each frequency spitting out a guess per denominator per t.  try that.


i think i need to do the activation test first.  ones and zeros.  no a.  cool

also.  is there a way to make a 1/0 target with that sigmoid thing?

add another one with a learnable c, and apply it to the target? the same c? idk.  weird.  

i think i need to make it so that the only branch that gets updated is the one with the activations.  so, forward prop the activations when you do grad updates? thats interesting.  



__________________________________________________
7/22
i don't think they can share weights honey.   make em individual.  that is still way low

ok, first it was doing real good at estimating a single non explicit frequency.  that is major.  

working on the non-linearity.  the shapes aren't doing what i think.  figure that bit out, the c's in there are being shared and broadcast.  i think what i saw with the single wave is the a's are unique, but the c's aren't.  so they all learn the same activation pattern at every level? that makes sense to me.  attack that. gj.  you did good today.


_______________________

7/21
original method didnt' work.  not surprising, added the waves at each layer, then. went to next.  i need branching

i might need to just parameterize the activation? move where the cutoff is? so i can do the 1s and 0s and all that?

each layer needs to learn its own amplitude, and that is added? at the end? yeah.  but it also needs to get passed through an activation that subsequent waves get selected with? or does every wave go through to the end? 



____________
7/20
hi sad boy.  

ok, double dutch layer.  takes a denominator, divides the sampling rate by that denom, and makes waves at opposite ends.  if you feed those into layers with succesivelye smaller denominators, you get binary double dutch.  

the amplitutdes.  if they are equal, they should cancel out.  that works.  if you make one higher than the other you get phase offsets.  

the other possible thing would be 2 sigmoids with positive 1 and negative 1.  not sure about that.  oooh actually, that and then a seperate param for that layers amplitude.  train and test babay.  
___________________________________________________________
7/16
sine wave layer higher has higher than sampled frequencies, the idea being since they only manifest discretely they can model different shapes.  interesting idea.  didn't work well with simple stuff.  
with complex stuff 

even with higher (than sample rate freq) waves it isn't filling in the gaps.  it can converge with perfect data, but estimating the non supplied freq is hard.  

i have SineWaveLayerInc working, and it seems to learn slow, but still improving

questioning mse, i want it to multiply the wave if it works, but average may not be working.  i want to not worry about a few big mistakes.  idk pondering.
also, add the high frequency ones too.  

what if we just add the 2 losses together, train them both?

___________________________________________________________________________________
7/15
no from Glidance.  fuck em

okay, distance from the cost is important.  if you do all the same it gives them all the same value.  random init?
network built them multiple times, not sure that helped.

the phase loss is working really well. add layer i think i prefer to the binary one.  thats neat

make it so the highest frequency is farthest away.  it starts by doing the lowest frequency wave in back prop, then moves back, the highest freq being the last.  so, each layer takes the next highest, adds it to the previous everything.  

can i rig that to adjust based on a local loss somehow? make each one substract the max that it can?

what happens if you add higher than sampling rate frequencies? i wonder if that gives you more control? like, you can estimate non-explicit frequencies using high freq waves? what would that look like? and what are the edits it would add.  draw.  it can't just be double, that would be the same as adding nyquist right?



_______________________________________
7/11

nyqvist frequecny



______________
7/10
OKAY the binary wave network is working.  right now it can learn itself.  I added phase agreement which is just trying to line phase up, that seems to be working well.  

phi_center isn't working like i think, but it is converging, just to a different phi minima, which is inconsequential. 

if the LEARNED wave is more limited that the target wave, it didn't converge.  that i think is the next big question.  if i add more frequencies can it learn? what if i did multiple of these and add them after the fact? CAPACITY????

can it learn, or rather, how well can it approximate waves that are composed of other frequencies.

explain the compression thing to kulis.  


_____________________________________
7/4
axis agreement might work for tuning phase.  HOWEVER, it depends on a correct frequency, otherwise maximizing this is nonsese.  
 messing with surrogates.  slope of all points? might work better with a range that is all connected?
____________________________________________________________________________________________
6/19
SINUSOIDAL FREQUENCY ESTIMATION BY GRADIENT DESCENT Hayes
	The loss (he shoes MSE, MAE, DFT [discreet Fourier transform] are all highly non convex (he is doing a sum of sin waves, I don't think my think differs substantially to, no, not sure if mine differs enough, I gotta math]. It is frequency that is the issue, the loss wrt frequency is going to be sin wavy, and those derivatives are going to be uninformative
	
	DIFFERENTIABLE DIGITAL SIGNAL PROCESSING engel
		At the heart of the synthesis techniques explored in this paper is the sinusoidal oscillator. That sounds like my first layer.
They individually model things, reverb, 
	multi scale spectrogram loss is what they are using.  

	frequency is the issue.  How can you tune it? Like, it's always wrong until it is exactly right.  If you are half the frequency, how can it learn to get to 3/4 then the right frequency.  It

	

_________________________________
6/17

alright, got it training.  the "batch" is really just how many time steps you run through it at the time.  i need to spend some time making sure it is doing what i think. 

got it training.  running it with mse, really high losses.  only have activations (relu) on w and phi, not on amplitude.  not sure about that.  

training with very few layers (the SineWaveNetwork() in csn) doesn't do much.  making it deeper, giving it more small waves, and decreasing to the one output that we overfitting on (dog bark, monet is the dog that inspired it, ben's dog, if you marry him, if not, delete)

make configs next time.  
hyperparameters = {
    'learning_rate': 0.0001,
    'epochs': 100,
    'batch_size': 100,
}
with (6 layers, decreasing from 100 base) started learning somewhat consistently

also, check the passing of the phi and w.  that addition (as well as the weird way i want to do gd)

you want the biggest waves and the longest phi to change the fastest.  thats gd.  

i think i can do it even with the relus on there.  mess with it.  yeah,  pre relu there will be negatives, which will subtract, after the backward pass and step, readjust everything
